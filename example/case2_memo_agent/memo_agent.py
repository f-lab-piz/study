"""LangGraph ê¸°ë°˜ ë©”ëª¨ ê´€ë¦¬ MCP ì—ì´ì „íŠ¸."""
from __future__ import annotations

import asyncio
import json
import os
from typing import Annotated, Any, Dict, List, cast

from dotenv import load_dotenv
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.tools import BaseTool
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.sessions import StreamableHttpConnection
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict

load_dotenv()

EXIT_KEYWORDS = {"exit", "quit", "ì¢…ë£Œ", "ê·¸ë§Œ", "ë", "q"}


class AgentState(TypedDict):
    """LangGraph ìƒíƒœ ì •ì˜."""

    messages: Annotated[List[BaseMessage], add_messages]


def _stringify(data: Any) -> str:
    if isinstance(data, str):
        return data
    try:
        return json.dumps(data, ensure_ascii=False, indent=2)
    except Exception:
        return str(data)


def _build_system_prompt(tool_map: Dict[str, BaseTool]) -> str:
    tool_lines = []
    for tool in tool_map.values():
        description = tool.description or "ì„¤ëª… ì—†ìŒ"
        tool_lines.append(f"- {tool.name}: {description}")
    tool_catalog = "\n".join(tool_lines)
    return (
        "ë„ˆëŠ” LangGraphë¡œ êµ¬í˜„ëœ ë©”ëª¨ ê´€ë¦¬ ì¡°ìˆ˜ë‹¤. "
        "FastAPI ë©”ëª¨ BEì™€ ì—°ê²°ëœ MCP ë„êµ¬ë§Œ ì‚¬ìš©í•˜ì—¬ ì‚¬ì‹¤ì„ í™•ì¸í•˜ê³  ìž‘ì—…ì„ ì²˜ë¦¬í•œë‹¤. "
        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µí•˜ë©°, ë„êµ¬ í˜¸ì¶œ ì´ìœ ì™€ ê²°ê³¼ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼. "
        "ì§ì ‘ ì¶”ì¸¡í•˜ì§€ ë§ê³  í•„ìš”í•œ ê²½ìš° ë„êµ¬ë¥¼ ë¨¼ì € í˜¸ì¶œí•œ ë’¤ ì‘ë‹µí•œë‹¤.\n\n"
        "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡:\n"
        f"{tool_catalog}\n\n"
        "ì‘ë‹µ ì§€ì¹¨:\n"
        "1) ì‚¬ìš©ìžê°€ ìš”ì²­í•œ ëª©í‘œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê³  í•„ìš”í•œ ê²½ìš° ì„¸ë¶€ ì •ë³´ë¥¼ ì§ˆë¬¸í•˜ë¼.\n"
        "2) ë„êµ¬ ê²°ê³¼ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ë§Œ ìš”ì•½í•˜ê³ , ì¶”ê°€ í›„ì† í–‰ë™ì„ ì œì•ˆí•˜ë¼.\n"
        "3) ìƒíƒœ ë³€í™” ìž‘ì—…(create/update/delete)ì„ ìˆ˜í–‰í–ˆìœ¼ë©´ ë³€ê²½ ë‚´ìš©ì„ ëª…í™•ížˆ ëª…ì‹œí•˜ë¼.\n"
        "4) ëŒ€í™”ê°€ ëë‚¬ë‹¤ê³  ëŠë¼ë”ë¼ë„ ì‚¬ìš©ìžê°€ 'ì¢…ë£Œ'ë¥¼ ìš”ì²­í•˜ê¸° ì „ê¹Œì§€ëŠ” ì¸ì‚¬ë¥¼ í•˜ê³  ë‹¤ìŒ ìž…ë ¥ì„ ê¸°ë‹¤ë¦¬ê² ë‹¤ê³  ì•Œë ¤ë¼.\n"
    )


def _create_graph(llm: ChatOpenAI, tool_map: Dict[str, BaseTool]) -> StateGraph:
    llm_with_tools = llm.bind_tools(list(tool_map.values()))

    async def call_model(state: AgentState) -> AgentState:
        response = await llm_with_tools.ainvoke(state["messages"])
        return {"messages": [response]}

    async def call_tools(state: AgentState) -> AgentState:
        last_message = state["messages"][-1]
        tool_calls = getattr(last_message, "tool_calls", None) or []
        outputs: List[ToolMessage] = []

        for call in tool_calls:
            tool_name = call.get("name")
            tool_args = call.get("args") or {}
            tool_call_id = call.get("id", tool_name or "tool-call")
            tool = tool_map.get(tool_name or "")
            if tool is None:
                error_text = f"{tool_name} ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                outputs.append(ToolMessage(content=error_text, tool_call_id=tool_call_id))
                continue

            print(f"ðŸ› ï¸  {tool_name} í˜¸ì¶œ: {_stringify(tool_args)}")
            try:
                result = await tool.ainvoke(tool_args)
                outputs.append(
                    ToolMessage(
                        content=_stringify(result),
                        tool_call_id=tool_call_id,
                    )
                )
                print(f"âœ… {tool_name} ê²°ê³¼: {_stringify(result)}")
            except Exception as exc:  # noqa: BLE001
                error_text = f"{tool_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {exc}"
                outputs.append(ToolMessage(content=error_text, tool_call_id=tool_call_id))
                print(f"âŒ {error_text}")

        return {"messages": outputs}

    def should_continue(state: AgentState) -> str:
        last_message = state["messages"][-1]
        if isinstance(last_message, AIMessage) and getattr(last_message, "tool_calls", None):
            return "tools"
        return "respond"

    builder = StateGraph(AgentState)
    builder.add_node("model", call_model)
    builder.add_node("tools", call_tools)

    builder.set_entry_point("model")
    builder.add_conditional_edges(
        "model",
        should_continue,
        {
            "tools": "tools",
            "respond": END,
        },
    )
    builder.add_edge("tools", "model")

    return builder.compile()


async def _ainput(prompt: str) -> str:
    return await asyncio.to_thread(input, prompt)


def _get_memo_mcp_url() -> str:
    explicit = os.getenv("MEMO_MCP_SERVER_URL") or os.getenv("MEMO_MCP_AGENT_URL")
    if explicit:
        return explicit
    host = os.getenv("MEMO_MCP_HOST", "127.0.0.1")
    port = os.getenv("MEMO_MCP_PORT", "8010")
    return f"http://{host}:{port}/mcp"


async def prepare_agent() -> tuple[StateGraph, AgentState]:
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise RuntimeError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

    server_name = os.getenv("MEMO_MCP_SERVER_NAME", "memo")
    server_url = _get_memo_mcp_url()
    connection = cast(
        StreamableHttpConnection,
        {
            "transport": "streamable_http",
            "url": server_url,
        },
    )
    client = MultiServerMCPClient({server_name: connection})
    tools = await client.get_tools(server_name=server_name)
    if not tools:
        raise RuntimeError("MCP ì„œë²„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    tool_map = {tool.name: tool for tool in tools}
    system_prompt = _build_system_prompt(tool_map)

    llm = ChatOpenAI(model=model_name, temperature=0.2)
    graph = _create_graph(llm, tool_map)

    base_state: AgentState = {
        "messages": [SystemMessage(content=system_prompt)],
    }
    return graph, base_state


async def run_cli() -> None:
    graph, state = await prepare_agent()
    print("âœ… LangGraph ë©”ëª¨ ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ', 'exit' ë“±ì„ ìž…ë ¥í•˜ì„¸ìš”)\n")

    while True:
        try:
            user_text = (await _ainput("ì‚¬ìš©ìž> ")).strip()
        except (EOFError, KeyboardInterrupt):
            print("\nðŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not user_text:
            continue

        if user_text.lower() in EXIT_KEYWORDS or user_text in EXIT_KEYWORDS:
            print("ðŸ‘‹ ìš”ì²­ì— ë”°ë¼ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        state["messages"].append(HumanMessage(content=user_text))
        try:
            new_state = await graph.ainvoke(state)
        except Exception as exc:  # noqa: BLE001
            print(f"âš ï¸  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {exc}")
            state["messages"].pop()
            continue

        state = {"messages": new_state["messages"]}
        ai_message = next(
            (msg for msg in reversed(state["messages"]) if isinstance(msg, AIMessage)),
            None,
        )
        if ai_message:
            print(f"ì—ì´ì „íŠ¸> {ai_message.content}\n")
        else:
            print("ì—ì´ì „íŠ¸> (ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)\n")

    print("âœ… ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ.")


def main() -> None:
    asyncio.run(run_cli())


if __name__ == "__main__":
    main()
