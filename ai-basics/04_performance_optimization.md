# AI ì„±ëŠ¥ ë° ë¹„ìš© ìµœì í™” - í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì í•„ìˆ˜ ê°€ì´ë“œ

AI APIëŠ” ì¼ë°˜ APIë³´ë‹¤ ëŠë¦¬ê³  ë¹„ìš©ì´ ë†’ìŠµë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìë¡œì„œ ì´ë¥¼ ì´í•´í•˜ê³  ìµœì í™” ì „ëµì„ ì œì•ˆí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

## 1. í´ë¼ì´ì–¸íŠ¸ vs ì„œë²„ ì¶”ë¡ 

### ë¹„êµ

| êµ¬ë¶„ | í´ë¼ì´ì–¸íŠ¸ ì¶”ë¡  | ì„œë²„ ì¶”ë¡  |
|------|----------------|----------|
| **ì†ë„** | ë¹ ë¦„ (ë„¤íŠ¸ì›Œí¬ ì—†ìŒ) | ëŠë¦¼ (ë„¤íŠ¸ì›Œí¬ ì§€ì—°) |
| **ë¹„ìš©** | ë¬´ë£Œ (ì‚¬ìš©ì ê¸°ê¸°) | ìœ ë£Œ (API í˜¸ì¶œ) |
| **ëª¨ë¸ í¬ê¸°** | ì‘ìŒ (< 100MB) | ì œí•œ ì—†ìŒ (ìˆ˜ì‹­ GB) |
| **ì •í™•ë„** | ë‚®ìŒ | ë†’ìŒ |
| **í”„ë¼ì´ë²„ì‹œ** | ìš°ìˆ˜ (ë¡œì»¬ ì²˜ë¦¬) | ë°ì´í„° ì „ì†¡ í•„ìš” |
| **ì—…ë°ì´íŠ¸** | ì–´ë ¤ì›€ | ì‰¬ì›€ (ì„œë²„ë§Œ) |

### í´ë¼ì´ì–¸íŠ¸ ì¶”ë¡ : TensorFlow.js

ê°„ë‹¨í•œ ì‘ì—…ì€ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```javascript
// ì´ë¯¸ì§€ ë¶„ë¥˜ (ë¸Œë¼ìš°ì €)
import * as mobilenet from '@tensorflow-models/mobilenet';

async function classifyImage(imageElement) {
    // ëª¨ë¸ ë¡œë“œ (ì²« ì‹¤í–‰ì‹œë§Œ, ì´í›„ ìºì‹±)
    const model = await mobilenet.load();

    // ì¶”ë¡  (ì¦‰ì‹œ ì‹¤í–‰, ì„œë²„ ë¶ˆí•„ìš”)
    const predictions = await model.classify(imageElement);

    console.log(predictions);
    // [
    //   { className: 'dog', probability: 0.92 },
    //   { className: 'cat', probability: 0.05 },
    //   ...
    // ]
}

// ì¥ì : ë¹ ë¦„, ë¬´ë£Œ, ì˜¤í”„ë¼ì¸ ê°€ëŠ¥
// ë‹¨ì : ì •í™•ë„ ë‚®ìŒ, ëª¨ë¸ í¬ê¸° ì œí•œ
```

### ì„œë²„ ì¶”ë¡ : API í˜¸ì¶œ

ë³µì¡í•œ ì‘ì—…ì€ ì„œë²„ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
# ì„œë²„ì—ì„œ ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
from transformers import pipeline

# GPUë¡œ ì‹¤í–‰ (ë¹ ë¥´ê³  ì •í™•)
classifier = pipeline("sentiment-analysis", model="bert-base-multilingual", device=0)

@app.post("/api/sentiment")
async def analyze_sentiment(text: str):
    result = classifier(text)
    return result
```

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

```javascript
// ê°„ë‹¨í•œ ì‘ì—…: í´ë¼ì´ì–¸íŠ¸
if (isSimpleTask) {
    result = await localModel.predict(input);
}
// ë³µì¡í•œ ì‘ì—…: ì„œë²„
else {
    result = await fetch('/api/ai/complex', { body: input });
}
```

**ì˜ˆì‹œ**:
- **í´ë¼ì´ì–¸íŠ¸**: ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•, ì–¼êµ´ ê°ì§€, ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜
- **ì„œë²„**: ë³µì¡í•œ í…ìŠ¤íŠ¸ ìƒì„±, ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±, ì •ë°€ ë¶„ì„

## 2. ìºì‹± ì „ëµ

AI APIëŠ” ëŠë¦¬ê³  ë¹„ì‹¸ë¯€ë¡œ, ê°™ì€ ìš”ì²­ì€ ìºì‹±í•´ì•¼ í•©ë‹ˆë‹¤.

### Redisë¡œ ì‘ë‹µ ìºì‹±

```python
# caching.py
import redis
import json
import hashlib
from openai import OpenAI

# Redis ì—°ê²°
redis_client = redis.Redis(host='localhost', port=6379, db=0)
openai_client = OpenAI()

def get_cache_key(prompt, model="gpt-3.5-turbo"):
    """ìºì‹œ í‚¤ ìƒì„±"""
    key_string = f"{model}:{prompt}"
    return hashlib.md5(key_string.encode()).hexdigest()

def chat_with_cache(prompt, ttl=3600):
    """ìºì‹±ì´ í¬í•¨ëœ ì±„íŒ…"""

    # 1. ìºì‹œ í™•ì¸
    cache_key = get_cache_key(prompt)
    cached = redis_client.get(cache_key)

    if cached:
        print("âœ“ ìºì‹œ íˆíŠ¸!")
        return json.loads(cached)

    # 2. ìºì‹œ ë¯¸ìŠ¤ - API í˜¸ì¶œ
    print("âœ— ìºì‹œ ë¯¸ìŠ¤ - API í˜¸ì¶œ")
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
    )

    result = {
        "response": response.choices[0].message.content,
        "tokens": response.usage.total_tokens,
        "cached": False
    }

    # 3. ìºì‹œ ì €ì¥ (1ì‹œê°„)
    redis_client.setex(
        cache_key,
        ttl,
        json.dumps(result, ensure_ascii=False)
    )

    return result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì²« ë²ˆì§¸ í˜¸ì¶œ: API í˜¸ì¶œ (ëŠë¦¼)
    result1 = chat_with_cache("Pythonì˜ ì¥ì ì„ 3ê°€ì§€ ë§í•´ì¤˜")
    print(result1["response"])

    # ë‘ ë²ˆì§¸ í˜¸ì¶œ: ìºì‹œ ì‚¬ìš© (ë¹ ë¦„)
    result2 = chat_with_cache("Pythonì˜ ì¥ì ì„ 3ê°€ì§€ ë§í•´ì¤˜")
    print(result2["response"])
```

### ì˜ë¯¸ ê¸°ë°˜ ìºì‹± (Semantic Caching)

ë¹„ìŠ·í•œ ì§ˆë¬¸ë„ ìºì‹±í•©ë‹ˆë‹¤.

```python
# semantic_caching.py
from openai import OpenAI
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import redis
import json

client = OpenAI()
redis_client = redis.Redis(decode_responses=True)

def get_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

def semantic_cache_lookup(query, similarity_threshold=0.9):
    """ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ìºì‹œ ê²€ìƒ‰"""

    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = get_embedding(query)

    # ëª¨ë“  ìºì‹œëœ ì¿¼ë¦¬ ê²€ìƒ‰
    cached_keys = redis_client.keys("semantic:*")

    best_match = None
    best_similarity = 0

    for key in cached_keys:
        cached_data = json.loads(redis_client.get(key))
        cached_embedding = cached_data["embedding"]

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(
            [query_embedding],
            [cached_embedding]
        )[0][0]

        if similarity > best_similarity and similarity >= similarity_threshold:
            best_similarity = similarity
            best_match = cached_data

    if best_match:
        print(f"âœ“ ì˜ë¯¸ ê¸°ë°˜ ìºì‹œ íˆíŠ¸! (ìœ ì‚¬ë„: {best_similarity:.2f})")
        return best_match["response"]

    return None

def chat_with_semantic_cache(query):
    """ì˜ë¯¸ ê¸°ë°˜ ìºì‹±"""

    # 1. ì˜ë¯¸ ê¸°ë°˜ ìºì‹œ í™•ì¸
    cached_response = semantic_cache_lookup(query)
    if cached_response:
        return {"response": cached_response, "cached": True}

    # 2. API í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": query}],
    )

    answer = response.choices[0].message.content

    # 3. ìºì‹œ ì €ì¥
    cache_key = f"semantic:{hash(query)}"
    cache_data = {
        "query": query,
        "embedding": get_embedding(query),
        "response": answer,
    }
    redis_client.setex(cache_key, 3600, json.dumps(cache_data))

    return {"response": answer, "cached": False}

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë‹¤ì–‘í•œ í‘œí˜„ì˜ ê°™ì€ ì§ˆë¬¸
    queries = [
        "Pythonì˜ ì¥ì ì„ ì•Œë ¤ì¤˜",
        "Pythonì´ ì¢‹ì€ ì´ìœ ëŠ” ë­ì•¼?",
        "íŒŒì´ì¬ì˜ ê°•ì ì€?",
    ]

    for q in queries:
        result = chat_with_semantic_cache(q)
        print(f"\nì§ˆë¬¸: {q}")
        print(f"ë‹µë³€: {result['response'][:100]}...")
        print(f"ìºì‹œë¨: {result['cached']}")
```

### FastAPIì—ì„œ ìºì‹± ì‚¬ìš©

```python
# main.py
from fastapi import FastAPI
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend
from fastapi_cache.decorator import cache
from redis import asyncio as aioredis

app = FastAPI()

@app.on_event("startup")
async def startup():
    redis = aioredis.from_url("redis://localhost")
    FastAPICache.init(RedisBackend(redis), prefix="fastapi-cache")

@app.post("/api/chat")
@cache(expire=3600)  # 1ì‹œê°„ ìºì‹±
async def chat(message: str):
    # ê°™ì€ messageëŠ” 1ì‹œê°„ ë™ì•ˆ ìºì‹±ë¨
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": message}],
    )
    return {"response": response.choices[0].message.content}
```

### í”„ë¡ íŠ¸ì—”ë“œ ìºì‹±

```javascript
// ë¸Œë¼ìš°ì € ìºì‹± (React Query)
import { useQuery } from '@tanstack/react-query';

function useChatQuery(message) {
    return useQuery({
        queryKey: ['chat', message],
        queryFn: async () => {
            const response = await fetch('/api/chat', {
                method: 'POST',
                body: JSON.stringify({ message }),
            });
            return response.json();
        },
        staleTime: 1000 * 60 * 60, // 1ì‹œê°„ ë™ì•ˆ ì‹ ì„ 
        cacheTime: 1000 * 60 * 60 * 24, // 24ì‹œê°„ ìºì‹±
    });
}

// ì‚¬ìš©
function ChatComponent() {
    const { data, isLoading } = useChatQuery("Python ì¥ì ì€?");

    // ê°™ì€ ì§ˆë¬¸ì„ ë‹¤ì‹œ í•˜ë©´ ì¦‰ì‹œ ì‘ë‹µ (ìºì‹œ ì‚¬ìš©)
}
```

## 3. Rate Limiting ë° ë°°ì¹˜ ì²˜ë¦¬

### Rate Limiting (ìš”ì²­ ì œí•œ)

API ì œê³µìì˜ ì œí•œì„ ì§€í‚¤ê³ , ë¹„ìš©ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
# rate_limiting.py
import time
from collections import deque
from threading import Lock

class RateLimiter:
    """ë¶„ë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ"""

    def __init__(self, max_requests, time_window=60):
        self.max_requests = max_requests  # ìµœëŒ€ ìš”ì²­ ìˆ˜
        self.time_window = time_window    # ì‹œê°„ ì°½ (ì´ˆ)
        self.requests = deque()
        self.lock = Lock()

    def acquire(self):
        """ìš”ì²­ í—ˆê°€ (í•„ìš”ì‹œ ëŒ€ê¸°)"""
        with self.lock:
            now = time.time()

            # ì˜¤ë˜ëœ ìš”ì²­ ì œê±°
            while self.requests and self.requests[0] < now - self.time_window:
                self.requests.popleft()

            # ì œí•œ ì´ˆê³¼ì‹œ ëŒ€ê¸°
            if len(self.requests) >= self.max_requests:
                wait_time = self.requests[0] + self.time_window - now
                print(f"Rate limit ë„ë‹¬. {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(wait_time)
                return self.acquire()  # ì¬ì‹œë„

            # ìš”ì²­ ê¸°ë¡
            self.requests.append(now)

# OpenAI ë¬´ë£Œ í‹°ì–´: ë¶„ë‹¹ 3íšŒ
rate_limiter = RateLimiter(max_requests=3, time_window=60)

def chat_with_rate_limit(message):
    """Rate limitingì´ ì ìš©ëœ ì±„íŒ…"""
    rate_limiter.acquire()  # í—ˆê°€ ëŒ€ê¸°

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": message}],
    )

    return response.choices[0].message.content

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # 4ê°œ ìš”ì²­ (4ë²ˆì§¸ëŠ” ëŒ€ê¸°)
    for i in range(4):
        print(f"\n{i+1}ë²ˆì§¸ ìš”ì²­")
        answer = chat_with_rate_limit(f"ìˆ«ì {i}ë¥¼ ì˜ì–´ë¡œ?")
        print(answer)
```

### ë°°ì¹˜ ì²˜ë¦¬

ì—¬ëŸ¬ ìš”ì²­ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë©´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

```python
# batch_processing.py
import asyncio
from typing import List

class BatchProcessor:
    """ìš”ì²­ì„ ëª¨ì•„ì„œ ë°°ì¹˜ ì²˜ë¦¬"""

    def __init__(self, batch_size=10, max_wait=2.0):
        self.batch_size = batch_size  # ë°°ì¹˜ í¬ê¸°
        self.max_wait = max_wait      # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        self.queue = []
        self.results = {}

    async def add_request(self, request_id, message):
        """ìš”ì²­ ì¶”ê°€"""
        future = asyncio.Future()
        self.queue.append((request_id, message, future))

        # ë°°ì¹˜ê°€ ì°¨ê±°ë‚˜ ì‹œê°„ ì´ˆê³¼ì‹œ ì²˜ë¦¬
        if len(self.queue) >= self.batch_size:
            await self.process_batch()

        # ë˜ëŠ” ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ í›„ ì²˜ë¦¬
        asyncio.create_task(self.auto_process(request_id))

        return await future

    async def auto_process(self, request_id):
        """ì¼ì • ì‹œê°„ í›„ ìë™ ì²˜ë¦¬"""
        await asyncio.sleep(self.max_wait)
        if any(req[0] == request_id for req in self.queue):
            await self.process_batch()

    async def process_batch(self):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        if not self.queue:
            return

        batch = self.queue[:self.batch_size]
        self.queue = self.queue[self.batch_size:]

        print(f"ë°°ì¹˜ ì²˜ë¦¬: {len(batch)}ê°œ ìš”ì²­")

        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for request_id, message, future in batch:
            tasks.append(self.process_single(request_id, message, future))

        await asyncio.gather(*tasks)

    async def process_single(self, request_id, message, future):
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬"""
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": message}],
            )
            result = response.choices[0].message.content
            future.set_result(result)
        except Exception as e:
            future.set_exception(e)

# FastAPIì—ì„œ ì‚¬ìš©
batch_processor = BatchProcessor(batch_size=10, max_wait=2.0)

@app.post("/api/chat/batch")
async def chat_batch(request_id: str, message: str):
    """ë°°ì¹˜ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    result = await batch_processor.add_request(request_id, message)
    return {"response": result}

# ì¥ì : 10ê°œ ìš”ì²­ì´ 2ì´ˆ ì•ˆì— ë“¤ì–´ì˜¤ë©´ í•œ ë²ˆì— ì²˜ë¦¬
# ë¹„ìš© ì ˆê° ë° ì²˜ë¦¬ëŸ‰ ì¦ê°€
```

### í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë””ë°”ìš´ì‹±

```javascript
// ì‚¬ìš©ìê°€ íƒ€ì´í•‘ ì¤‘ì¼ ë•ŒëŠ” ìš”ì²­ ì•ˆ ë³´ë‚´ê¸°
import { useState, useEffect } from 'react';

function useDebouncedValue(value, delay = 500) {
    const [debouncedValue, setDebouncedValue] = useState(value);

    useEffect(() => {
        const handler = setTimeout(() => {
            setDebouncedValue(value);
        }, delay);

        return () => clearTimeout(handler);
    }, [value, delay]);

    return debouncedValue;
}

// ì‚¬ìš©
function SearchWithAI() {
    const [query, setQuery] = useState('');
    const debouncedQuery = useDebouncedValue(query, 1000);

    useEffect(() => {
        if (debouncedQuery) {
            // 1ì´ˆ ë™ì•ˆ ì…ë ¥ ì—†ìœ¼ë©´ ê²€ìƒ‰
            fetchAISearch(debouncedQuery);
        }
    }, [debouncedQuery]);

    return (
        <input
            value={query}
            onChange={(e) => setQuery(e.target.value)}
            placeholder="AI ê²€ìƒ‰..."
        />
    );
}
```

## 4. ëª¨ë¸ ê²½ëŸ‰í™” ì´í•´

ML íŒ€ê³¼ í˜‘ì—…í•  ë•Œ ì•Œì•„ë‘ë©´ ì¢‹ìŠµë‹ˆë‹¤.

### ëª¨ë¸ í¬ê¸° vs ì„±ëŠ¥

```
GPT-4 (1.7T íŒŒë¼ë¯¸í„°)
- í¬ê¸°: ~ìˆ˜ TB
- ì„±ëŠ¥: ìµœê³ 
- ì†ë„: ëŠë¦¼ (10-30ì´ˆ)
- ë¹„ìš©: ë§¤ìš° ë¹„ìŒˆ

GPT-3.5 (175B íŒŒë¼ë¯¸í„°)
- í¬ê¸°: ~350GB
- ì„±ëŠ¥: ìš°ìˆ˜
- ì†ë„: ë¹ ë¦„ (1-3ì´ˆ)
- ë¹„ìš©: ì €ë ´

DistilBERT (66M íŒŒë¼ë¯¸í„°)
- í¬ê¸°: ~250MB
- ì„±ëŠ¥: ë³´í†µ
- ì†ë„: ë§¤ìš° ë¹ ë¦„ (< 100ms)
- ë¹„ìš©: ê±°ì˜ ë¬´ë£Œ
```

### í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìê°€ ì œì•ˆí•  ìˆ˜ ìˆëŠ” ê²ƒ

```python
# ML íŒ€ê³¼ì˜ ëŒ€í™” ì˜ˆì‹œ

í”„ë¡ íŠ¸ì—”ë“œ: "í˜„ì¬ ì‘ë‹µ ì‹œê°„ì´ 5ì´ˆì¸ë°, 3ì´ˆ ì´ë‚´ë¡œ ì¤„ì¼ ìˆ˜ ìˆì„ê¹Œìš”?"

ML ì—”ì§€ë‹ˆì–´: "ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ëª¨ë¸ ì–‘ìí™”ë¥¼ ì ìš©í•  ìˆ˜ ìˆì–´ìš”."

í”„ë¡ íŠ¸ì—”ë“œ: "ì •í™•ë„ê°€ ì–¼ë§ˆë‚˜ ë–¨ì–´ì§ˆê¹Œìš”?"

ML ì—”ì§€ë‹ˆì–´: "í˜„ì¬ 92%ì¸ë°, 89%ë¡œ ì•½ê°„ ë–¨ì–´ì§ˆ ê²ƒ ê°™ì•„ìš”."

í”„ë¡ íŠ¸ì—”ë“œ: "UX í…ŒìŠ¤íŠ¸ ê²°ê³¼ 3% ì°¨ì´ëŠ” ì‚¬ìš©ìë“¤ì´ ëª» ëŠê¼ˆì–´ìš”.
           ì†ë„ê°€ ë” ì¤‘ìš”í•˜ë‹ˆ ì ìš©í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”."
```

### ì–‘ìí™” (Quantization) ì˜ˆì‹œ

```python
# ëª¨ë¸ ì–‘ìí™”ë¡œ í¬ê¸°ì™€ ì†ë„ ê°œì„ 
from transformers import AutoModelForCausalLM
import torch

# ì›ë³¸ ëª¨ë¸ (FP32)
model = AutoModelForCausalLM.from_pretrained("gpt2")
# í¬ê¸°: ~500MB, ì†ë„: 100ms

# ì–‘ìí™” (INT8)
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)
# í¬ê¸°: ~125MB (4ë°° ê°ì†Œ), ì†ë„: 50ms (2ë°° ë¹ ë¦„)
# ì •í™•ë„: ê±°ì˜ ë™ì¼ (~1% ì°¨ì´)
```

## 5. ë¹„ìš© ìµœì í™”

### ëª¨ë¸ ì„ íƒ ì „ëµ

```python
# ì‘ì—…ë³„ ëª¨ë¸ ì„ íƒ
def choose_model(task_complexity, budget="low"):
    """ì‘ì—… ë³µì¡ë„ì™€ ì˜ˆì‚°ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""

    if budget == "low":
        if task_complexity == "simple":
            return "gpt-3.5-turbo"  # $0.0015/1K tokens
        elif task_complexity == "medium":
            return "gpt-3.5-turbo"
        else:
            return "gpt-4-turbo"    # $0.03/1K tokens (í•„ìš”ì‹œë§Œ)

    elif budget == "high":
        if task_complexity == "simple":
            return "gpt-3.5-turbo"
        else:
            return "gpt-4"          # $0.06/1K tokens

    return "gpt-3.5-turbo"  # ê¸°ë³¸ê°’

# ì‚¬ìš© ì˜ˆì‹œ
@app.post("/api/chat/smart")
async def smart_chat(message: str, task_type: str):
    """ì‘ì—…ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ"""

    # ê°„ë‹¨í•œ FAQ â†’ ì €ë ´í•œ ëª¨ë¸
    if task_type == "faq":
        model = "gpt-3.5-turbo"

    # ë³µì¡í•œ ì½”ë“œ ìƒì„± â†’ ê°•ë ¥í•œ ëª¨ë¸
    elif task_type == "code_generation":
        model = "gpt-4-turbo"

    # ì¼ë°˜ ëŒ€í™” â†’ ì¤‘ê°„ ëª¨ë¸
    else:
        model = "gpt-3.5-turbo"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": message}],
    )

    return {
        "response": response.choices[0].message.content,
        "model_used": model,
        "cost": calculate_cost(model, response.usage.total_tokens)
    }
```

### í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¡œ í† í° ì ˆì•½

```python
# âŒ ë‚˜ìœ ì˜ˆ: ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ í”„ë¡¬í”„íŠ¸
bad_prompt = """
ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ì§€ê¸ˆ Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ê³µë¶€í•˜ê³  ìˆëŠ” í•™ìƒì…ë‹ˆë‹¤.
Pythonì— ëŒ€í•´ì„œ ë°°ìš°ê³  ìˆëŠ”ë°ìš”, ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì— ëŒ€í•´ì„œ ì˜ ëª¨ë¥´ê² ì–´ì„œ
ì´ë ‡ê²Œ ì§ˆë¬¸ì„ ë“œë¦½ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€ìš”?
ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.
"""
# í† í°: ~60

# âœ… ì¢‹ì€ ì˜ˆ: ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
good_prompt = "Python ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ë¥¼ 3ê°€ì§€ë¡œ ìš”ì•½í•´ì¤˜"
# í† í°: ~15

# 75% í† í° ì ˆì•½!
```

### ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²´ê° ì†ë„ ê°œì„ 

```python
# ìŠ¤íŠ¸ë¦¬ë°: ì²« í† í° 0.5ì´ˆ, ì „ì²´ 5ì´ˆ
# ì¼ë°˜: ì²« ì‘ë‹µ 5ì´ˆ

# ì‚¬ìš©ìëŠ” ìŠ¤íŠ¸ë¦¬ë°ì´ í›¨ì”¬ ë¹ ë¥´ê²Œ ëŠë‚Œ!
# ë¹„ìš©ì€ ë™ì¼í•˜ì§€ë§Œ UX ê°œì„ 
```

### ë¹„ìš© ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
# cost_monitoring.py
from datetime import datetime, timedelta

class CostMonitor:
    """ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼"""

    def __init__(self, daily_limit=100.0):
        self.daily_limit = daily_limit  # ì¼ì¼ í•œë„ (USD)
        self.today_cost = 0.0
        self.today = datetime.now().date()

    def add_cost(self, cost):
        """ë¹„ìš© ì¶”ê°€"""
        # ë‚ ì§œ ë³€ê²½ì‹œ ë¦¬ì…‹
        if datetime.now().date() != self.today:
            self.today_cost = 0.0
            self.today = datetime.now().date()

        self.today_cost += cost

        # ê²½ê³  ìˆ˜ì¤€
        usage_percent = (self.today_cost / self.daily_limit) * 100

        if usage_percent >= 90:
            self.alert("ğŸš¨ ì¼ì¼ ë¹„ìš© 90% ì´ˆê³¼!")
        elif usage_percent >= 75:
            self.alert("âš ï¸ ì¼ì¼ ë¹„ìš© 75% ì´ˆê³¼")

        # í•œë„ ì´ˆê³¼ì‹œ ì°¨ë‹¨
        if self.today_cost >= self.daily_limit:
            raise Exception("ì¼ì¼ ë¹„ìš© í•œë„ ì´ˆê³¼")

    def alert(self, message):
        """ì•Œë¦¼ ì „ì†¡"""
        print(f"[{datetime.now()}] {message}")
        print(f"ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰: ${self.today_cost:.2f} / ${self.daily_limit}")
        # ì‹¤ì œë¡œëŠ” Slack, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì „ì†¡

cost_monitor = CostMonitor(daily_limit=100.0)

@app.post("/api/chat/monitored")
async def monitored_chat(message: str):
    """ë¹„ìš© ëª¨ë‹ˆí„°ë§ì´ ì ìš©ëœ ì±„íŒ…"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": message}],
    )

    # ë¹„ìš© ê³„ì‚° ë° ê¸°ë¡
    cost = calculate_cost("gpt-3.5-turbo", response.usage.total_tokens)
    cost_monitor.add_cost(cost)

    return {
        "response": response.choices[0].message.content,
        "cost": cost,
        "daily_usage": cost_monitor.today_cost,
        "daily_limit": cost_monitor.daily_limit,
    }
```

## 6. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©

ì„ë² ë”© ê²€ìƒ‰ì€ ì „ìš© DBë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë¹ ë¦…ë‹ˆë‹¤.

```python
# vector_db.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Qdrant (ë²¡í„° DB) ì—°ê²°
qdrant = QdrantClient("localhost", port=6333)

# ì»¬ë ‰ì…˜ ìƒì„±
qdrant.create_collection(
    collection_name="faq",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
)

def add_faq_to_db(question, answer):
    """FAQë¥¼ ë²¡í„° DBì— ì¶”ê°€"""
    # ì„ë² ë”© ìƒì„±
    embedding = get_embedding(question)

    # ì €ì¥
    qdrant.upsert(
        collection_name="faq",
        points=[
            PointStruct(
                id=hash(question) % (10 ** 8),
                vector=embedding,
                payload={"question": question, "answer": answer}
            )
        ]
    )

def search_similar_faq(query, top_k=3):
    """ìœ ì‚¬í•œ FAQ ê²€ìƒ‰"""
    query_embedding = get_embedding(query)

    results = qdrant.search(
        collection_name="faq",
        query_vector=query_embedding,
        limit=top_k,
    )

    return [
        {
            "question": hit.payload["question"],
            "answer": hit.payload["answer"],
            "score": hit.score,
        }
        for hit in results
    ]

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # FAQ ì¶”ê°€
    add_faq_to_db(
        "ë°°ì†¡ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?",
        "ì¼ë°˜ ë°°ì†¡ì€ 3-5ì¼ ì†Œìš”ë©ë‹ˆë‹¤."
    )

    # ê²€ìƒ‰ (ë§¤ìš° ë¹ ë¦„: < 10ms)
    results = search_similar_faq("ë°°ì†¡ ê¸°ê°„ì´ ê¶ê¸ˆí•´ìš”")
    print(results)
    # [
    #   {
    #     "question": "ë°°ì†¡ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?",
    #     "answer": "ì¼ë°˜ ë°°ì†¡ì€ 3-5ì¼ ì†Œìš”ë©ë‹ˆë‹¤.",
    #     "score": 0.95
    #   }
    # ]
```

**ì¥ì **:
- ì¼ë°˜ DB (PostgreSQL): 1000ê°œ ê²€ìƒ‰ ~500ms
- ë²¡í„° DB (Qdrant): 100ë§Œê°œ ê²€ìƒ‰ ~10ms

### ì¸ë±ì‹± ì „ëµ

```python
# MongoDBì— ì„ë² ë”© ì €ì¥ ë° ì¸ë±ì‹±
from pymongo import MongoClient

mongo = MongoClient()
db = mongo["ai_app"]

# ì¸ë±ìŠ¤ ìƒì„± (ì‘ë‹µ ì†ë„ í–¥ìƒ)
db.conversations.create_index([("user_id", 1), ("timestamp", -1)])
db.feedbacks.create_index([("message_id", 1)])

# ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¿¼ë¦¬ ìµœì í™”
def get_user_conversations(user_id, limit=10):
    """ì‚¬ìš©ì ëŒ€í™” ê¸°ë¡ (ì¸ë±ìŠ¤ í™œìš©)"""
    return db.conversations.find(
        {"user_id": user_id}
    ).sort("timestamp", -1).limit(limit)
```

## 7. ì‹¤ì „ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### í”„ë¡ íŠ¸ì—”ë“œ

```javascript
// 1. ë””ë°”ìš´ì‹±ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ìš”ì²­ ë°©ì§€
const debouncedSearch = useDebouncedValue(searchQuery, 1000);

// 2. ìš”ì²­ ì·¨ì†Œ (ì‚¬ìš©ìê°€ ìƒˆ ê²€ìƒ‰ ì‹œì‘í•˜ë©´ ì´ì „ ìš”ì²­ ì·¨ì†Œ)
const abortController = new AbortController();
fetch('/api/search', {
    signal: abortController.signal
});

// 3. ë‚™ê´€ì  ì—…ë°ì´íŠ¸
setMessages([...messages, { text: userInput, from: 'user' }]);
// API í˜¸ì¶œ ì „ì— UI ë¨¼ì € ì—…ë°ì´íŠ¸

// 4. ìŠ¤ì¼ˆë ˆí†¤ UIë¡œ ì²´ê° ì†ë„ ê°œì„ 
{isLoading && <SkeletonLoader />}

// 5. ì—ëŸ¬ ë°”ìš´ë”ë¦¬
<ErrorBoundary fallback={<ErrorUI />}>
    <AIFeature />
</ErrorBoundary>
```

### ë°±ì—”ë“œ

```python
# 1. ìºì‹±
@cache(expire=3600)
async def chat(message: str): ...

# 2. Rate limiting
@limiter.limit("10/minute")
async def chat(message: str): ...

# 3. ë°°ì¹˜ ì²˜ë¦¬
await batch_processor.add_request(id, message)

# 4. ë¹„ë™ê¸° ì²˜ë¦¬
async def chat(message: str):
    async with httpx.AsyncClient() as client:
        response = await client.post(...)

# 5. ëª¨ë‹ˆí„°ë§
from prometheus_client import Counter
api_calls = Counter('ai_api_calls', 'AI API í˜¸ì¶œ ìˆ˜')
api_calls.inc()

# 6. íƒ€ì„ì•„ì›ƒ ì„¤ì •
response = await asyncio.wait_for(
    call_ai_api(message),
    timeout=30.0
)
```

## ìš”ì•½

### ìµœì í™” ìš°ì„ ìˆœìœ„

1. **ìºì‹±** (ê°€ì¥ í° íš¨ê³¼)
   - Redisë¡œ ì¤‘ë³µ ìš”ì²­ ì œê±°
   - ì˜ë¯¸ ê¸°ë°˜ ìºì‹±ìœ¼ë¡œ ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬

2. **Rate Limiting**
   - API ì œí•œ ì¤€ìˆ˜
   - ë¹„ìš© ê´€ë¦¬

3. **ë°°ì¹˜ ì²˜ë¦¬**
   - ì—¬ëŸ¬ ìš”ì²­ ë¬¶ì–´ì„œ ì²˜ë¦¬
   - ì²˜ë¦¬ëŸ‰ ì¦ê°€

4. **ëª¨ë¸ ì„ íƒ**
   - ì‘ì—…ì— ë§ëŠ” ëª¨ë¸ ì‚¬ìš©
   - ê°„ë‹¨í•œ ì‘ì—…ì€ ì €ë ´í•œ ëª¨ë¸

5. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**
   - ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
   - ë¶ˆí•„ìš”í•œ í† í° ì œê±°

### ì„±ëŠ¥ ëª©í‘œ

```
ì‘ë‹µ ì‹œê°„:
- ìºì‹œ íˆíŠ¸: < 100ms âš¡
- ê°„ë‹¨í•œ ì‘ì—…: < 2ì´ˆ âœ“
- ë³µì¡í•œ ì‘ì—…: < 5ì´ˆ â±
- ìŠ¤íŠ¸ë¦¬ë°: ì²« í† í° < 1ì´ˆ âš¡

ë¹„ìš©:
- ì‚¬ìš©ìë‹¹ í•˜ë£¨: < $0.10
- í•œ ë‹¬ ì´ ë¹„ìš©: ì˜ˆì‚° ë‚´

ì •í™•ë„:
- ì‚¬ìš©ì ë§Œì¡±ë„: > 80%
- ê¸ì • í”¼ë“œë°±: > 70%
```

### ML íŒ€ê³¼ í˜‘ì—…í•˜ê¸°

```
í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìê°€ ì œê³µí•  ì •ë³´:
âœ“ ì‚¬ìš©ì í”¼ë“œë°± ë° ë¶ˆë§Œ ì‚¬í•­
âœ“ ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ë°ì´í„°
âœ“ ì‘ë‹µ ì†ë„ì— ëŒ€í•œ UX í…ŒìŠ¤íŠ¸ ê²°ê³¼
âœ“ ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ì˜ê²¬
âœ“ ë¹„ìš© ìµœì í™” ì•„ì´ë””ì–´

ML ì—”ì§€ë‹ˆì–´ê°€ ì œê³µí•  ì •ë³´:
âœ“ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ
âœ“ ìµœì í™” ê°€ëŠ¥ ì—¬ë¶€
âœ“ ì˜ˆìƒ ë¹„ìš© ë° ì†ë„
âœ“ ëª¨ë¸ í•œê³„ ë° ì œì•½ì‚¬í•­
```

ì´ì œ AI í”„ë¡œì íŠ¸ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í˜‘ì—…í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!
