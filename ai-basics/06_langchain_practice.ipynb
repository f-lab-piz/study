{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LangChain ê¸°ì´ˆ ì‹¤ìŠµ\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- LangChainìœ¼ë¡œ LLMì„ ì¶”ìƒí™”í•˜ì—¬ ì‚¬ìš©í•˜ê¸°\n",
    "- Prompt Template, Output Parser, Chain ì‚¬ìš©í•˜ê¸°\n",
    "- ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ë©€í‹°í„´ ì±—ë´‡ ë§Œë“¤ê¸°\n",
    "- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import load_dotenv, find_dotenv\n\n# find_dotenv(): í˜„ì¬ ë””ë ‰í† ë¦¬ë¶€í„° ìƒìœ„ë¡œ ì˜¬ë¼ê°€ë©° .env íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ìŒ\n# í”„ë¡œì íŠ¸ ë£¨íŠ¸(/workspaces/study/.env)ì— .env íŒŒì¼ì„ ë‘ë©´ ì–´ë””ì„œë“  ë™ì‘\nload_dotenv(find_dotenv())\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    print(\"âš ï¸ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ì„ ìƒì„±í•˜ê³  OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”\")\nelse:\n    print(\"âœ… API í‚¤ ë¡œë“œ ì™„ë£Œ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM ì¶”ìƒí™” í´ë˜ìŠ¤\n",
    "\n",
    "LangChainì€ ë‹¤ì–‘í•œ LLM APIë¥¼ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# ê°„ë‹¨í•œ í˜¸ì¶œ\n",
    "response = llm.invoke(\"Pythonì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì¤˜\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë‹¤ì–‘í•œ LLM ì œê³µì\n",
    "\n",
    "ê°™ì€ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ë¥¸ LLMë„ ì‚¬ìš© ê°€ëŠ¥:\n",
    "\n",
    "```python\n",
    "# OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Anthropic Claude\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n",
    "\n",
    "# ì‚¬ìš©ë²•ì€ ë™ì¼!\n",
    "response = llm.invoke(\"Hello\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Template\n",
    "\n",
    "í”„ë¡¬í”„íŠ¸ë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# í…œí”Œë¦¿ ì •ì˜\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"{text}ë¥¼ {language}ë¡œ ë²ˆì—­í•´ì¤˜\"\n",
    ")\n",
    "\n",
    "# í…œí”Œë¦¿ í¬ë§·íŒ…\n",
    "formatted = prompt.format(text=\"Hello World\", language=\"í•œêµ­ì–´\")\n",
    "print(formatted)\n",
    "print()\n",
    "\n",
    "# LLMì— ì „ë‹¬\n",
    "response = llm.invoke(formatted)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate (ì‹œìŠ¤í…œ + ì‚¬ìš©ì ë©”ì‹œì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì¹œì ˆí•œ {role} ì„ ìƒë‹˜ì´ì•¼.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    role=\"Python\",\n",
    "    question=\"ë°ì½”ë ˆì´í„°ê°€ ë­ì•¼?\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Parser\n",
    "\n",
    "LLMì˜ ë¬¸ìì—´ ì¶œë ¥ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LLM ì‘ë‹µì„ ë¬¸ìì—´ë¡œ íŒŒì‹±\n",
    "response = llm.invoke(\"Hi there!\")\n",
    "parsed = parser.parse(response)\n",
    "print(type(parsed))  # <class 'str'>\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic Output Parser (êµ¬ì¡°í™”ëœ ë°ì´í„°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"ì‚¬ëŒì˜ ì´ë¦„\")\n",
    "    age: int = Field(description=\"ì‚¬ëŒì˜ ë‚˜ì´\")\n",
    "    occupation: str = Field(description=\"ì§ì—…\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì— í¬ë§· ì§€ì‹œ í¬í•¨\n",
    "prompt = PromptTemplate(\n",
    "    template=\"ë‹¤ìŒ ì¸ë¬¼ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜:\\n{format_instructions}\\n\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({\"query\": \"ì² ìˆ˜ëŠ” 30ì‚´ì´ê³  ê°œë°œìë¡œ ì¼í•˜ê³  ìˆë‹¤.\"})\n",
    "\n",
    "print(f\"ì´ë¦„: {result.name}\")\n",
    "print(f\"ë‚˜ì´: {result.age}\")\n",
    "print(f\"ì§ì—…: {result.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chain (ì²´ì¸)\n",
    "\n",
    "LCEL(LangChain Expression Language)ë¡œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic}ì— ëŒ€í•´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜\")\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„± (íŒŒì´í”„ ì—°ì‚°ì)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# ì‹¤í–‰\n",
    "result = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìˆœì°¨ ì²´ì¸ (Sequential Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ë‹¨ê³„: ì•„ì´ë””ì–´ ìƒì„±\n",
    "idea_prompt = ChatPromptTemplate.from_template(\n",
    "    \"{industry} ë¶„ì•¼ì˜ ìŠ¤íƒ€íŠ¸ì—… ì•„ì´ë””ì–´ë¥¼ í•˜ë‚˜ ì œì‹œí•´ì¤˜\"\n",
    ")\n",
    "idea_chain = idea_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 2ë‹¨ê³„: ì•„ì´ë””ì–´ ë¶„ì„\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"ë‹¤ìŒ ì•„ì´ë””ì–´ë¥¼ ë¶„ì„í•´ì¤˜:\\n{idea}\\n\\nì¥ë‹¨ì ì„ ì•Œë ¤ì¤˜.\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì „ì²´ ì²´ì¸ ì¡°í•©\n",
    "full_chain = (\n",
    "    {\"idea\": idea_chain}\n",
    "    | analysis_chain\n",
    ")\n",
    "\n",
    "result = full_chain.invoke({\"industry\": \"í—¬ìŠ¤ì¼€ì–´\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë©”ëª¨ë¦¬ (Memory)\n",
    "\n",
    "LLMì´ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# ëŒ€í™” ì €ì¥\n",
    "memory.save_context(\n",
    "    {\"input\": \"ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼\"},\n",
    "    {\"output\": \"ì•ˆë…•í•˜ì„¸ìš” ì² ìˆ˜ë‹˜!\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?\"},\n",
    "    {\"output\": \"ì² ìˆ˜ë‹˜ì´ë¼ê³  í•˜ì…¨ì–´ìš”.\"}\n",
    ")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ í™•ì¸\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationChain (ëŒ€í™” ì²´ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False  # Trueë¡œ í•˜ë©´ ë‚´ë¶€ ë™ì‘ í™•ì¸ ê°€ëŠ¥\n",
    ")\n",
    "\n",
    "# ëŒ€í™”\n",
    "print(\"Bot:\", conversation.predict(input=\"ë‚´ ì´ë¦„ì€ ì˜í¬ì•¼\"))\n",
    "print()\n",
    "print(\"Bot:\", conversation.predict(input=\"ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?\"))\n",
    "print()\n",
    "print(\"Bot:\", conversation.predict(input=\"ë°©ê¸ˆ ë­˜ ë¬¼ì–´ë´¤ì–´?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë©€í‹°í„´ ì±—ë´‡\n",
    "\n",
    "ì™„ì „í•œ ëŒ€í™”í˜• ì±—ë´‡ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# ì„¤ì •\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ëŒ€í™”\n",
    "test_messages = [\n",
    "    \"ì•ˆë…•! ë‚˜ëŠ” Python ë°°ìš°ëŠ” ì¤‘ì´ì•¼\",\n",
    "    \"ë°ì½”ë ˆì´í„°ê°€ ë­”ì§€ ì•Œë ¤ì¤„ë˜?\",\n",
    "    \"ì˜ˆì œ ì½”ë“œë„ ë³´ì—¬ì¤˜\",\n",
    "    \"ë‚´ê°€ ë­˜ ë°°ìš°ê³  ìˆë‹¤ê³  í–ˆì§€?\"\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    print(f\"You: {msg}\")\n",
    "    response = conversation.predict(input=msg)\n",
    "    print(f\"Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Memory (ìµœê·¼ Kê°œë§Œ ê¸°ì–µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# ìµœê·¼ 2ê°œ ëŒ€í™”ë§Œ ìœ ì§€\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "conversation.predict(input=\"ë©”ì‹œì§€ 1\")\n",
    "conversation.predict(input=\"ë©”ì‹œì§€ 2\")\n",
    "conversation.predict(input=\"ë©”ì‹œì§€ 3\")  # ë©”ì‹œì§€ 1ì€ ìŠí˜€ì§\n",
    "print(window_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic}ì— ëŒ€í•œ ì§§ì€ ì´ì•¼ê¸°ë¥¼ ì¨ì¤˜\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë°\n",
    "for chunk in chain.stream({\"topic\": \"ë¡œë´‡\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "### Level 1\n",
    "1. PromptTemplateìœ¼ë¡œ ë‹¤êµ­ì–´ ë²ˆì—­ê¸° ë§Œë“¤ê¸° (í•œêµ­ì–´â†’ì˜ì–´, ì˜ì–´â†’ì¼ë³¸ì–´ ë“±)\n",
    "2. PydanticOutputParserë¡œ ì˜í™” ì •ë³´(ì œëª©, ê°ë…, ì¥ë¥´, ê°œë´‰ë…„ë„) ì¶”ì¶œí•˜ê¸°\n",
    "3. ConversationChainìœ¼ë¡œ ê°„ë‹¨í•œ ì±—ë´‡ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ì œ 1: ë‹¤êµ­ì–´ ë²ˆì—­ê¸°\n",
    "# TODO: ì½”ë“œ ì‘ì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2\n",
    "4. ìˆœì°¨ ì²´ì¸ìœ¼ë¡œ \"í‚¤ì›Œë“œ ìƒì„± â†’ ë¸”ë¡œê·¸ ì œëª© ìƒì„± â†’ ë³¸ë¬¸ ì‘ì„±\" íŒŒì´í”„ë¼ì¸\n",
    "5. ConversationBufferWindowMemoryë¡œ ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ê¸°ì–µí•˜ëŠ” ì±—ë´‡\n",
    "6. ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê¸´ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” Q&A ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ì œ 2: ë¸”ë¡œê·¸ ì‘ì„± íŒŒì´í”„ë¼ì¸\n",
    "# TODO: ì½”ë“œ ì‘ì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 3\n",
    "7. Few-shot í”„ë¡¬í”„íŠ¸ë¡œ ê°ì„± ë¶„ì„ê¸° ë§Œë“¤ê¸°\n",
    "8. ì„¸ì…˜ë³„ë¡œ ëŒ€í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” ë©€í‹° ìœ ì € ì±—ë´‡\n",
    "9. ë³‘ë ¬ ì²´ì¸ìœ¼ë¡œ ê°™ì€ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ëª¨ë¸ì— ë™ì‹œì— ë³´ë‚´ê³  ê²°ê³¼ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ì œ 3: Few-shot ê°ì„± ë¶„ì„\n",
    "# TODO: ì½”ë“œ ì‘ì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "LangChainì˜ ì¥ì :\n",
    "- âœ… í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ LLM ì‚¬ìš©\n",
    "- âœ… í”„ë¡¬í”„íŠ¸ë¥¼ í…œí”Œë¦¿ìœ¼ë¡œ ì¬ì‚¬ìš©\n",
    "- âœ… ì¶œë ¥ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ íŒŒì‹±\n",
    "- âœ… ì²´ì¸ìœ¼ë¡œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±\n",
    "- âœ… ë©”ëª¨ë¦¬ë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì´ì œ Langfuseë¡œ LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ëª¨ë‹ˆí„°ë§í•´ë´…ì‹œë‹¤!\n",
    "\n",
    "ğŸ‘‰ [07_langfuse_practice.ipynb](07_langfuse_practice.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}